{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Linear Regression](#toc0_)\n",
    "\n",
    "There are several formulations of linear regression. This includes\n",
    "\n",
    "- [Ordinary Least Squares](https://en.wikipedia.org/wiki/Ordinary_least_squares)\n",
    "\n",
    "- [Weighted Least Squares](https://en.wikipedia.org/wiki/Weighted_least_squares)\n",
    "\n",
    "- [Generalized Least Squares](https://en.wikipedia.org/wiki/Generalized_least_squares)\n",
    "\n",
    "- [Iteratively Reweighted Least Squares](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares) \n",
    "\n",
    "- [Instrumental Variables Regression](https://en.wikipedia.org/wiki/Instrumental_variables_estimation) \n",
    "\n",
    "- [Total Least Squares](https://en.wikipedia.org/wiki/Total_least_squares) \n",
    "\n",
    "- Linear Template Fit \n",
    "\n",
    "- Percentage Least Squares\n",
    "\n",
    "- [Constrained Least Squares](https://en.wikipedia.org/wiki/Constrained_least_squares)\n",
    "\n",
    "We will concentrate on Ordinary Least Squares, which is the most commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "\n",
    "We have $ n $ data points from observations $ (x_1, y_1) \\ (x_2, y_2) \\ \\cdots (x_n, y_n) $. Where $ x $ is the predictor variable and $ y $ is the response variable. These can also be considered as vectors $ \\textbf{x} $ and $ \\textbf{y} $.\n",
    "\n",
    "We wish to fit a linear model through this data that satisfies\n",
    "\n",
    "$ y_i = \\beta_0 + \\beta_1 x_i + e_i \\ \\ \\ \\ $ where $ \\ 0 \\lt i \\le n $\n",
    "\n",
    "so we have $ n $ equations\n",
    "\n",
    "$ y_1 = \\beta_0 + \\beta_1 x_1 + e_1 \\ \\ \\ \\ \\\\ $ \n",
    "\n",
    "$ y_2 = \\beta_0 + \\beta_1 x_2 + e_2 \\ \\ \\ \\ \\\\ $ \n",
    "\n",
    "$ \\vdots \\\\ $\n",
    "\n",
    "$ y_n = \\beta_0 + \\beta_1 x_n + e_n \\ \\ \\ \\ $ \n",
    "\n",
    "Where $ \\beta_0 $ and $ \\beta_1 $ are model parameters. $ e_i $ is called the error term and is defined as $ e_i = y_i - (\\beta_0 + \\beta_1 x_i) $\n",
    "\n",
    "In matrix notation:\n",
    "\n",
    "$ \\textbf{y} = \\textbf{x} \\boldsymbol{\\beta} + \\textbf{e} $\n",
    "\n",
    "where\n",
    "\n",
    "$\n",
    "\\textbf{y} =\n",
    "\\begin{bmatrix}\n",
    "\\ \\ y_1 \\ \\ \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "\\ \\ \\ \\ \\ \\\n",
    "\\textbf{x} =\n",
    "\\begin{bmatrix}\n",
    "\\ \\ 1 \\ \\ \\ x_1 \\ \\\\\n",
    "\\ \\ 1 \\ \\ \\ x_2 \\ \\\\\n",
    "\\ \\vdots \\ \\ \\\\\n",
    "\\ \\ 1 \\ \\ x_n\n",
    "\\end{bmatrix}\n",
    "\\ \\ \\ \\ \\ \\\n",
    "\\boldsymbol{\\beta} =\n",
    "\\begin{bmatrix}\n",
    "\\ \\ \\beta_1 \\ \\ \\\\\n",
    "\\beta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_n\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "or\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "\\ \\ y_1 \\ \\ \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "    =\n",
    "\\begin{bmatrix}\n",
    "\\ \\ \\beta_0 + \\beta_1 x_1 \\ \\\\\n",
    "\\ \\ \\beta_0 + \\beta_1 x_2 \\ \\\\\n",
    "\\ \\vdots \\ \\ \\\\\n",
    "\\ \\ \\beta_0 + \\beta_1 x_n\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "At each data point, the error of prediction is given by\n",
    "\n",
    "$ e_i(\\beta) = y_i - x_i \\beta \\ \\ \\ \\ \\ $ for $ \\ \\ 0 < i \\le n $\n",
    "\n",
    "or in matrix form\n",
    "\n",
    "$ \\textbf{e}(\\beta) = \\textbf{y} - \\textbf{x}\\beta $\n",
    "\n",
    "We define the mean squared error as follows:\n",
    "\n",
    "$ MSE(\\beta) = \\dfrac{1}{n} \\sum \\limits_{i=1}^{n} e_i^2 (\\beta) $\n",
    "\n",
    "or in matrix notation\n",
    "\n",
    "$ MSE(\\beta) = \\dfrac{1}{n} \\textbf{e}^T \\textbf{e} $\n",
    "\n",
    "$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =  \\dfrac{1}{n} (\\textbf{y} - \\textbf{x}\\boldsymbol{\\beta})^T (\\textbf{y} - \\textbf{x}\\boldsymbol{\\beta}) $\n",
    "\n",
    "$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =  \\dfrac{1}{n} (\\textbf{y}^T - \\boldsymbol{\\beta}^T \\textbf{x}^T) (\\textbf{y} - \\textbf{x}\\boldsymbol{\\beta}) $\n",
    "\n",
    "$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =  \\dfrac{1}{n} (\\textbf{y}^T \\textbf{y} - \\textbf{y}^T \\textbf{x} \\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T \\textbf{x}^T \\textbf{y} + \\boldsymbol{\\beta}^T \\textbf{x}^T \\textbf{x}\\boldsymbol{\\beta}) $\n",
    "\n",
    "<br>\n",
    "\n",
    "Now $ (\\textbf{y}^T \\textbf{x} \\boldsymbol{\\beta})^T = \\textbf{y}^T \\textbf{x} \\boldsymbol{\\beta} = \\boldsymbol{\\beta}^T \\textbf{x}^T \\textbf{y} $\n",
    "\n",
    "$ \\therefore MSE(\\boldsymbol{\\beta}) = \\dfrac{1}{n} (\\textbf{y}^T \\textbf{y} - 2\\boldsymbol{\\beta}^T \\textbf{x}^T \\textbf{y} + \\boldsymbol{\\beta}^T \\textbf{x}^T \\textbf{x}\\boldsymbol{\\beta}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares\n",
    "\n",
    "The method of least squares is a parameter estimation method based on minimizing the sum of the squares of the residuals, $ MSE(\\boldsymbol{\\beta}) $, the difference between observed values and fitted values provided by a model, made in the results of each individual equation.\n",
    "\n",
    "First, we find the gradient of the mean square error, MSE with respect to $ \\boldsymbol{\\beta} $\n",
    "\n",
    "$ \\nabla MSE(\\boldsymbol{\\beta}) = \\dfrac{1}{n} (\\nabla \\boldsymbol{y}^T \\textbf{y} - 2\\nabla\\boldsymbol{\\beta}^T \\textbf{x}^T \\textbf{y} + \\nabla\\boldsymbol{\\beta}^T \\textbf{x}^T \\textbf{x}\\boldsymbol{\\beta}) $\n",
    "\n",
    "$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\dfrac{1}{n} (\\nabla \\textbf{y}^T \\textbf{y} - 2\\nabla\\boldsymbol{\\beta}^T \\textbf{x}^T \\textbf{y} + \\nabla\\boldsymbol{\\beta}^T \\textbf{x}^T \\textbf{x}\\boldsymbol{\\beta}) $\n",
    "\n",
    "$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\dfrac{1}{n} (0 - 2 \\textbf{x}^T \\textbf{y} + 2 \\textbf{x}^T \\textbf{x}\\boldsymbol{\\beta}) $\n",
    "\n",
    "$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\dfrac{2}{n} (\\textbf{x}^T \\textbf{x}\\boldsymbol{\\beta} - \\textbf{x}^T \\textbf{y}) $\n",
    "\n",
    "To find the minimum, set this equal to zero\n",
    "\n",
    "$ \\textbf{x}^T \\textbf{x}\\boldsymbol{\\beta} - \\textbf{x}^T \\textbf{y} = 0 $\n",
    "\n",
    "The solution of this equation provides us with the optimized values for $ \\hat{\\boldsymbol{\\beta_0}} $ and $ \\hat{\\boldsymbol{\\beta_1}} $\n",
    "\n",
    "Rearranging, we get\n",
    "\n",
    "$ \\hat{\\boldsymbol{\\beta}} = (\\textbf{x}^T \\textbf{x})^{-1}  \\textbf{x}^T \\textbf{y} $\n",
    "\n",
    "Introduce a normalizing factor of $ 1/n $\n",
    "\n",
    "$ \\hat{\\boldsymbol{\\beta}} = \\dfrac{n}{n}\\Big(\\textbf{x}^T \\textbf{x}\\Big)^{-1}  \\textbf{x}^T \\textbf{y} = \\Big(\\dfrac{1}{n}\\textbf{x}^T \\textbf{x}\\Big)^{-1} \\Big(\\dfrac{1}{n}\\textbf{x}^T \\textbf{y}\\Big)$\n",
    "\n",
    "Now the second term \n",
    "\n",
    "$ \n",
    "\\dfrac{1}{n}\\textbf{x}^T \\textbf{y} \n",
    "    = \\dfrac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "  1 & 1 & \\cdots & 1 \\\\\n",
    "  x_1 & x_2 & \\cdots & x_n \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "  \\ y_1 \\ \\\\\n",
    "  \\ y_2 \\ \\\\\n",
    "  \\ \\vdots \\ \\\\\n",
    "  \\ y_n \\\n",
    "\\end{bmatrix}\n",
    "    =\n",
    "\\begin{bmatrix}\n",
    "  y_1 & y_2 & \\cdots & y_n \\\\\n",
    "  x_1 y_1 & x_2 y_2 & \\cdots & x_n y_n \n",
    "\\end{bmatrix}\n",
    "    =\n",
    "\\begin{bmatrix}\n",
    "  \\sum_{i}^{} y_i \\\\\n",
    "  \\sum_{i}^{} x_i y_i  \n",
    "\\end{bmatrix}\n",
    "    =\n",
    "\\begin{bmatrix}\n",
    "  \\bar{y} \\\\\n",
    "  \\overline{xy}  \n",
    "\\end{bmatrix}\n",
    "$ \n",
    "\n",
    "Similarly for the first term\n",
    "\n",
    "$ \n",
    "\\dfrac{1}{n}\\textbf{x}^T \\textbf{x} \n",
    "    = \\dfrac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "  1 & 1 & \\cdots & 1 \\\\\n",
    "  x_1 & x_2 & \\cdots & x_n \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "  1 & x_1 \\\\\n",
    "  1 & x_2 \\\\\n",
    "  \\ \\vdots \\\\\n",
    "  1 & x_n \n",
    "\\end{bmatrix}\n",
    "    =\n",
    "\\dfrac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "  1 + 1 \\cdots + 1 & x_1 + x_2 + \\cdots + x_n \\\\\n",
    "  x_1 + x_2 + \\cdots + x_n & x_1^2 + x_2^2 + \\cdots + x_n^2\n",
    "\\end{bmatrix}\n",
    "    =\n",
    "\\begin{bmatrix}\n",
    "  1 & \\bar{x} \\\\\n",
    "  \\bar{x} & \\bar{x}^2 \n",
    "\\end{bmatrix}\n",
    "$ \n",
    "\n",
    "$\n",
    "\\Rightarrow \\Big(\\dfrac{1}{n}\\textbf{x}^T \\textbf{x}\\Big)^{-1}\n",
    "    =\n",
    "\\dfrac{1}{\\overline{x^2} - \\bar{x}^2}\n",
    "\\begin{bmatrix}\n",
    "  \\overline{x^2} & -\\bar{x} \\\\\n",
    "  -\\bar{x} & 1  \n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Multiply the first and second terms\n",
    "\n",
    "$\n",
    "\\hat{\\beta} =\n",
    "\\Big(\\dfrac{1}{n}\\textbf{x}^T \\textbf{x}\\Big)^{-1} \\Big(\\dfrac{1}{n}\\textbf{x}^T \\textbf{y}\\Big)\n",
    "    = \n",
    "\\dfrac{1}{\\overline{x^2} - \\bar{x}^2}\n",
    "\\begin{bmatrix}\n",
    "  \\overline{x^2} & -\\bar{x} \\\\\n",
    "  -\\bar{x} & 1  \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "  \\bar{y} \\\\\n",
    "  \\overline{xy}  \n",
    "\\end{bmatrix}\n",
    "    =\n",
    "\\dfrac{1}{\\overline{x^2} - \\bar{x}^2}\n",
    "\\begin{bmatrix}\n",
    "  \\bar{x^2} \\bar{y} - \\overline{xxy} \\\\\n",
    "  -\\bar{x}\\bar{y}+\\overline{xy}\n",
    "\\end{bmatrix}\n",
    "    =\n",
    "\\dfrac{1}{\\overline{x^2} - \\bar{x}^2}\n",
    "\\begin{bmatrix}\n",
    "  \\bar{x^2} \\bar{y} - \\overline{xxy} \\\\\n",
    "  \\overline{xy}-\\bar{x}\\bar{y}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Now the variance is given by $ s_x^2 = \\overline{x^2} - \\bar{x}^2 $\n",
    " \n",
    "and the covariance is given by $ c_{xy} = \\overline{xy} - \\bar{x}\\bar{y} $\n",
    "\n",
    "$\n",
    "\\hat{\\beta}\n",
    "    =\n",
    "\\dfrac{1}{s_x^2}\n",
    "\\begin{bmatrix}\n",
    "  (s_x^2 + \\bar{x}^2)\\bar{y} - \\bar{x}(c_{xy} + \\bar{x}\\bar{y}) \\\\\n",
    "  c_{xy}\n",
    "\\end{bmatrix}\n",
    "    =\n",
    "\\begin{bmatrix}\n",
    "  s_x^2 \\bar{y} + \\bar{x}^2 \\bar{y} - \\bar{x}c_{xy} + \\bar{x}^2\\bar{y} \\\\\n",
    "  c_{xy}\n",
    "\\end{bmatrix}\n",
    "    =\n",
    "\\begin{bmatrix}\n",
    "  \\bar{y} - \\dfrac{\\bar{x}c_{xy}}{s_x^2} \\\\\n",
    "  c_{xy}\n",
    "\\end{bmatrix} \n",
    "$\n",
    "\n",
    "$\n",
    "\\therefore \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\n",
    "$\n",
    "\n",
    "and\n",
    "\n",
    "$\n",
    "\\hat{\\beta_1} = \\dfrac{\\overline{xy} -\\bar{x}\\bar{y}}{\\overline{x^2} - \\bar{x}^2}\n",
    "$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
